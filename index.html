<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.54.0" />
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>Ian Duffy | Rants of a software engineer</title>

  
  <link rel="stylesheet" href="/css/poole.css">
  <link rel="stylesheet" href="/css/hyde.css">
  <link rel="stylesheet" href="/css/poole-overrides.css">
  <link rel="stylesheet" href="/css/hyde-overrides.css">
  <link rel="stylesheet" href="/css/hyde-x.css">
  <link rel="stylesheet" href="/css/highlight/sunburst.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
  <link href="/favicon.png" rel="icon">

  
  
  
  <link href="http://ianduffy.ie/index.xml" rel="alternate" type="application/rss+xml" title="Ian Duffy &middot; Ian Duffy" />

  <meta name="description" content="Your default page description">
  <meta name="keywords" content="your,default,page,keywords">
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-33060296-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      
      <h1>Ian Duffy</h1>
      <p class="lead">Rants of a software engineer</p>
    </div>

    <ul class="sidebar-nav">
      
    </ul>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <a href="http://github.com/imduffy15"><i class="fa fa-github-square fa-3x"></i></a>
      
      
      <a href="https://ie.linkedin.com/in/imduffy15"><i class="fa fa-linkedin-square fa-3x"></i></a>
      
      
      
      <a href="https://www.youtube.com/channel/UCoGs2iiOIGrfXofp-3g-Qqg"><i class="fa fa-youtube-square fa-3x"></i></a>
      
      </li>
    </ul>

    
  </div>
</div>


<div class="content container">
  <div class="posts">
    
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2019/02/22/local-development-with-virtual-hosts-and-https/">Local development with virtual hosts and HTTPS</a>
      </h1>
      <span class="post-date">Feb 22, 2019 &middot; 4 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/development">Development</a><a class="label" href="http://ianduffy.ie/categories/web">Web</a><a class="label" href="http://ianduffy.ie/categories/docker">Docker</a>
      </span>
      
      

<p><center><img src="/images/local-development-with-virtual-hosts-and-https/diagram.png" alt="" /></center></p>

<p>When doing development locally it might be necessary to access the application(s) using a virtual host (vhost) and/or HTTPS. This post describes an approach to achieving this on OSX using Docker, which avoids creating a large mess on your computer.</p>

<h2 id="domain-name-systems-dns">Domain Name Systems (DNS)</h2>

<p>Domain Name Systems (DNS) are often referred to as the phonebook of the internet. People access information online through domain names, like ‘amazon.com’ or ‘rte.ie’. DNS is responsible for translating a domain name to an IP address.</p>

<p>When an HTTP resource is accessed using DNS, an additional header containing the domain name is provided to the HTTP server. The HTTP server uses this for routing the request to the correct vhost.</p>

<p>In order to have functioning vhosts, DNS is required. <a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">Dnsmasq</a> is a popular DNS server, it can be configured to resolve a set of specified domains to a specified IP address. Using docker a dnsmasq server can easily be created:</p>

<pre><code>$ docker run -it -p 53:53/udp --cap-add=NET_ADMIN andyshinn/dnsmasq:latest --log-queries --log-facility=- --address=/dev.ianduffy.ie/127.0.0.1
</code></pre>

<p>The provided command line arguments instruct dnsmasq to resolve all requests for dev.ianduffy.ie and *.dev.ianduffy.ie to 127.0.0.1. Using dig - a DNS lookup utility this configuration can be validated:</p>

<pre><code>$ dig @127.0.0.1 dev.ianduffy.ie +short
127.0.0.1
</code></pre>

<p>Additionally, all wildcards of dev.ianduffy.ie also resolve:</p>

<pre><code>$ dig @127.0.0.1 random-string.dev.ianduffy.ie +short
127.0.0.1
</code></pre>

<p>While this works, the OSX is not configured to use this DNS server for it’s lookups so attempting to resolve dev.ianduffy.ie in the browser will fail. It&rsquo;s possible to specify nameservers for a specific domain name, this can be achieved by creating a file within /etc/resolver with a filename that matches the domain.</p>

<p>For example, if /etc/resolver/dev.ianduffy.ie is created with the following contents:</p>

<pre><code>nameserver 127.0.0.1
</code></pre>

<p>Now all queries to dev.ianduffy.ie will be resolved by doing a lookup against the DNS server at 127.0.0.1.</p>

<h2 id="http">HTTP</h2>

<p>A HTTP server will be required for routing the requests based on a vhost and supplying HTTPS. Nginx is a good fit for this, even-more-so as <a href="https://www.linkedin.com/in/jason-wilder-94549/">Jason Wilder</a> from Microsoft has created a  <a href="https://github.com/jwilder/nginx-proxy">container image</a> that exposes the nginx reverse proxy functionality via environment variables.</p>

<p><a href="https://github.com/jwilder/nginx-proxy">nginx-proxy</a> can be started using the following:</p>

<pre><code>$ docker run -it -p 80:80 -v /var/run/docker.sock:/tmp/docker.sock:ro jwilder/nginx-proxy
</code></pre>

<p><a href="https://github.com/jwilder/nginx-proxy">nginx-proxy</a> will look for <code>VIRTUAL_HOST</code> environment variables on other docker containers and route to them accordingly. To demonstrate this, a container running <a href="https://httpbin.org">httpbin</a> which provides data for debugging HTTP requests can be created, with a VIRTUAL_HOST environment variable specified.</p>

<pre><code>$ docker run -e VIRTUAL_HOST=httpbin.dev.ianduffy.ie kennethreitz/httpbin
</code></pre>

<p>This service can now be accessed via httpbin.dev.ianduffy.ie. Alternatively, if you do not have the dnsmasq service from earlier running, the service can be accessed by passing the header &ldquo;host&rdquo; with value &ldquo;httpbin.dev.ianduffy.ie&rdquo;. This can be tested with an HTTP client like <a href="https://curl.haxx.se/">curl</a></p>

<pre><code>$ curl http://httpbin.dev.ianduffy.ie/headers
{
  &quot;headers&quot;: {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Connection&quot;: &quot;close&quot;,
    &quot;Host&quot;: &quot;httpbin.dev.ianduffy.ie&quot;,
    &quot;User-Agent&quot;: &quot;curl/7.54.0&quot;
  }
}
</code></pre>

<pre><code>$ curl -H &quot;host: httpbin.dev.ianduffy.ie&quot; http://127.0.0.1/headers
{
  &quot;headers&quot;: {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Connection&quot;: &quot;close&quot;,
    &quot;Host&quot;: &quot;httpbin.dev.ianduffy.ie&quot;,
    &quot;User-Agent&quot;: &quot;curl/7.54.0&quot;
  }
}
</code></pre>

<h2 id="https">HTTPS</h2>

<p>In some scenarios, HTTPS might be required. <a href="https://github.com/FiloSottile/mkcert">mkcert</a> provides locally trusted SSL certificates and automatically OSX, Linux, and Windows system stores along with Firefox, Chrome and Java.</p>

<p>With <a href="https://github.com/FiloSottile/mkcert">mkcert</a> installed, certificates can be generated with the following command:</p>

<pre><code>$ mkdir certs
$ mkcert -cert-file certs/dev.ianduffy.ie.crt -key-file certs/dev.ianduffy.ie.key -install dev.ianduffy.ie *.dev.ianduffy.ie
</code></pre>

<p>By mounting these certificates, as a volume on a container running nginx-proxy HTTPS will be enabled.</p>

<pre><code>$ docker run -it -p 80:80 -p 443:443 -v $(pwd)/certs:/etc/nginx/certs -v /var/run/docker.sock:/tmp/docker.sock:ro jwilder/nginx-proxy
</code></pre>

<p>Executing <code>curl</code> to <a href="https://httpbin.dev.ianduffy.ie">https://httpbin.dev.ianduffy.ie</a> will now respond successfully. Additionally, the <code>-v</code> flag can be specified to tell curl to be verbose and it will display information about the SSL certificate.</p>

<pre><code>$ curl -v https://httpbin.dev.ianduffy.ie/headers
</code></pre>

<h2 id="placing-a-vhost-in-front-of-a-local-application">Placing a vhost in front of a local application</h2>

<p>Most developers will want to run their application on the host and continue with the standard development workflow. To accommodate for this, a container that forwards traffic to a host port can be created.</p>

<p><a href="http://www.dest-unreach.org/socat/doc/README">socat</a> is perfect for this use case, it enables us to specify a source IP address and port and a destination IP address and port. In the example below, all traffic for test.dev.ianduffy.ie will be forwarded to the docker host on port 9000.</p>

<pre><code>$ docker run -it --expose 80 -e VIRTUAL_HOST=test.dev.ianduffy.ie alpine/socat tcp-listen:80,fork,reuseaddr tcp-connect:host.docker.internal:9000
</code></pre>

<h2 id="bring-it-all-together-with-docker-compose">Bring it all together with docker-compose</h2>

<p>All of the containers described above can be brought together in a single docker compose file. This provides ease of bringing the system up with a single command.</p>

<pre><code>nginx-proxy:
  container_name: nginx-proxy
  image: jwilder/nginx-proxy
  ports:
    - 80:80
    - 443:443
  volumes:
    - /var/run/docker.sock:/tmp/docker.sock:ro
    - ./certs:/etc/nginx/certs:ro
dnsmasq:
  container_name: dnsmasq
  image: andyshinn/dnsmasq:latest
  command: --log-queries --log-facility=- --address=/dev.ianduffy.ie/127.0.0.1
  ports:
    - 53:53
    - 53:53/udp
  cap_add:
    - NET_ADMIN
socat:
  image: alpine/socat
  command: tcp-listen:80,fork,reuseaddr tcp-connect:host.docker.internal:9000
  environment:
    - VIRTUAL_HOST=dev.ianduffy.ie
    - VIRTUAL_PORT=80
  expose:
    - 80
</code></pre>

<p>This enables VHosts and HTTPS for applications running on the host without creating too much of a mess as its all contained within Docker.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2018/12/15/managing-access-to-multiple-aws-accounts-with-openid/">Managing access to multiple AWS Accounts with OpenID</a>
      </h1>
      <span class="post-date">Dec 15, 2018 &middot; 6 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/cloud">Cloud</a><a class="label" href="http://ianduffy.ie/categories/security">Security</a><a class="label" href="http://ianduffy.ie/categories/aws">AWS</a><a class="label" href="http://ianduffy.ie/categories/serverless">Serverless</a>
      </span>
      
      

<p>Many organisations look towards a multiple account strategy with Amazon Web Services (AWS) to provide administrative isolation between workloads, limited visibility and discoverability of workloads, isolation to minimize blast radius, management of AWS limits and cost categorisation. However, this comes at a large complexity cost, specifically around Identity Access Management (IAM).</p>

<p>Starting off with a single AWS account, and using a handful of IAM users and groups for access management, is usually the norm. As an organisation grows they start to see a need for separate staging, production, and developer tooling accounts. Managing access to these can quickly become a mess. Do you create a unique IAM user in each account and provide your employees with the unique sign-on URL? Do you create a single IAM user for each employee and use <a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"><code>AssumeRole</code></a> to generate credentials and to enable jumping between accounts? How do employees use the AWS Application Programming Interface (API) or the Command Line Interface (CLI); are long-lived access keys generated? How is an employee&rsquo;s access revoked should they leave the organisation?</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_0.jpg" alt="" /></p>

<p>User per account approach</center></p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_1.jpg" alt="" /></p>

<p>All users in a single account</p>

<p>using STS AssumeRole to access other accounts</center></p>

<h1 id="design">Design</h1>

<h2 id="reuse-employees-existing-identities">Reuse employees existing identities</h2>

<p>In most organisations, an employee will already have an identity, normally used for accessing e-mail. These identities are normally stored in Active Directory (AD), Google Suite (GSuite) or Office 365. In an ideal world, these identities could be reused and would grant access to AWS. This means employees would only need to remember one set of credentials and their access could be revoked from a single place.</p>

<h2 id="expose-an-openid-compatible-interface-for-authentication">Expose an OpenID compatible interface for authentication</h2>

<p>OpenID provides applications with a way to verify a users identity using JSON Web Tokens (JWT). Additionally, it provides profile information about the end user such as first name, last name, email address, group membership, etc. This profile information can be used to store the AWS accounts and AWS roles the user has access to.</p>

<p>By placing an OpenID compatible interface on top an organisation&rsquo;s identity store users can easily generate JWTs which can be later used by services to authenticate them.</p>

<h2 id="trading-jwts-for-aws-api-credentials">Trading JWTs for AWS API Credentials</h2>

<p>In order to trade JWTs for AWS API Credentials, a service can be created that runs on AWS with a role that has access to AssumeRole.This service would be responsible for validating a users JWT, ensuring the JWT contains the requested role and executing STS AssumeRole to generate the AWS API Credentials.</p>

<p>Additionally, the service would also generate an <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html">Amazon Federated Sign-On URL</a> which would enable users to access the AWS Web Console using their JWT.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_2.png" alt="" /></center></p>

<h1 id="example-implementation">Example Implementation</h1>

<p>Provided below is an example implementation of the above design. One user with username &ldquo;demo&rdquo; and password “demo” exists. Please do not use this demo in a production environment without https.</p>

<p>To follow along, clone or <a href="https://github.com/imduffy15/aws-credentials-issuer/archive/master.zip">download</a> the code at <a href="https://github.com/imduffy15/aws-credentials-issuer">https://github.com/imduffy15/aws-credentials-issuer</a>.</p>

<h2 id="openid-provider">OpenID Provider</h2>

<p><a href="https://www.keycloak.org/">Keycloak</a> provides an IAM along with OpenID Connect (OIDC) and Security Assertion Markup Language (SAML) interfaces. Additionally, it supports <a href="https://www.keycloak.org/docs/3.0/server_admin/topics/user-federation.html">federation</a> to Active Directory (AD) and Lightweight Directory Access Protocol (LDAP) servers. Keycloak enables organisations to centrally manage employees access to many different services.</p>

<p>The<a href="https://github.com/imduffy15/aws-credentials-issuer"> provided code</a> contains a <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/docker-compose.yaml">docker-compose file</a> that on executing docker-compose up will bring up a keycloak server with its administrative interface accessible at <a href="http://localhost:8080/auth/admin/">http://localhost:8080/auth/admin/</a> using username &ldquo;admin&rdquo; and password “password”.</p>

<p>On the &ldquo;clients&rdquo; screen, a client named “aws-credentials-issuer” is present, users will use this client to generate their JWT tokens. This client is pre-configured to work with the <a href="https://tools.ietf.org/html/rfc6749#section-4.1">Authorization Code Grant</a> for command line interfaces to generate tokens and the <a href="https://tools.ietf.org/html/rfc6749#section-4.2">Implicit Grant</a> for a frontend application.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_3.png" alt="" /></center></p>

<p>Under the &ldquo;aws-credentials-issuer&rdquo; additional roles can be added, these roles must exist on AWS and they must have a trust relationship to the account that will be running the “aws-credentials-issuer”.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_4.png" alt="" /></center></p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_5.png" alt="" /></center></p>

<p>Additionally, these roles must be placed into the users JWT tokens, this is pre-configured under &ldquo;mappers&rdquo;.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_6.png" alt="" /></center></p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_7.png" alt="" /></center></p>

<p>Finally, the role must be assigned to a user. This can be done by navigating to users -&gt; demo -&gt; role mappings and moving the wanted role from &ldquo;available roles&rdquo; to “assigned roles” for the client “aws-credentials-issuer”</p>

<p><img src="/images/managing-access-to-multiple-aws-accounts/image_8.png" alt="" /></p>

<h2 id="aws-credentials-issuer-service">AWS Credentials Issuer Service</h2>

<h3 id="backend">Backend</h3>

<p>The <a href="https://github.com/imduffy15/aws-credentials-issuer">provided code</a> supplies a <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/lambda_handler.py">lambda function</a> which will take care of validating the users JWT token and exchanging it using <code>AssumeRole</code> for AWS Credentials.</p>

<p>This code can be deployed to an AWS account by using the <a href="https://serverless.com/">serverless framework</a> and the <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml">supplied definition</a>. The <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml">definition</a> will create the following:</p>

<ul>
<li><p>An <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L36">IAM role</a> granting AssumeRole rights to the function</p></li>

<li><p>A <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/lambda_handler.py#L67">function</a> for generating AWS API Credentials</p></li>

<li><p>A <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/lambda_handler.py#L36">function</a> for generating <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html">Amazon Federated Sign-On URL</a>s</p></li>
</ul>

<p>With <a href="https://aws.amazon.com/cli/">AWSCLI</a> configured with credentials for the account that the service will run in execute <code>sls deploy</code>, this will deploy the lambda functions and return URLs for executing them.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_9.png" alt="" /></center></p>

<h3 id="frontend">Frontend</h3>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_10.png" alt="" /></center></p>

<p>The <a href="https://github.com/imduffy15/aws-credentials-issuer">provided code</a> supplies a <a href="https://github.com/imduffy15/aws-credentials-issuer/tree/master/ui">frontend</a> which will provide users with a graphical experience for accessing the AWS Web Console or generating AWS API Credentials.</p>

<p>The frontend can be deployed to an S3 bucket using the <a href="https://serverless.com/">serverless framework</a>. Before deploying it some variables must be modified. In the <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L75">serverless definition (</a><a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L75"><code>serverless.yml</code></a><a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L75">)</a>, replace &ldquo;ianduffy-aws-credentials-issuer&rdquo; with a desired S3 bucket name and modify <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/ui/.env"><code>ui/.env</code></a> to contain your Keycloak and Backend URL as highlighted above. The deployment can be executed with <code>sls client deploy</code>.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_11.png" alt="" /></center></p>

<p>On completion, a URL in the format of <code>http://&lt;bucket-name&gt;.s3-website.&lt;region&gt;.amazonaws.com</code> will be returned. This needs to be supplied to keycloak a redirect URI for the &ldquo;aws-credentials-issuer&rdquo; client.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_12.png" alt="" /></center></p>

<h2 id="usage">Usage</h2>

<h3 id="browser">Browser</h3>

<p>By navigating to the URL of the S3 bucket a user can get access to the AWS Web Console or get API credentials which they can use to manually configure an application.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_13.gif" alt="" /></center></p>

<h3 id="command-line">Command line</h3>

<p>To interact with the &ldquo;aws-credentials-issuer&rdquo; the user must have a valid JWT. This can be done by executing the <a href="https://tools.ietf.org/html/rfc6749#section-4.1">Authorization Code Grant</a> against keycloak.</p>

<p>token-cli can be used to execute the  <a href="https://tools.ietf.org/html/rfc6749#section-4.1">Authorization Code Grant</a> and generate a JWT token, this can be downloaded from the projects <a href="https://github.com/imduffy15/token-cli/releases/tag/v0.0.3">releases page</a>; alternatively, on OSX it can be installed with homebrew brew install imduffy15/tap/token-cli.</p>

<p>Once token-cli is installed it must be configured to run against keycloak, this can be done as follows:</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_14.png" alt="" /></center></p>

<p>Finally, a token can be generated with token-cli token get aws-credentials-issuer -p 9000. On first run the users browser will be opened and they will be required to login, on subsequent runs the token will be cached or refreshed automatically.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_15.gif" alt="" /></center></p>

<p>This token can be used against the &ldquo;aws-credentials-issuer&rdquo; to get AWS API credentials:</p>

<pre><code>curl https://&lt;API-GATEWAY&gt;/dev/api/credentials?role=&lt;ROLE-ARN&gt; \
-H &quot;Authorization: bearer $(token-cli token get aws-credentials-issuer)&quot;
</code></pre>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_16.png" alt="" /></center></p>

<p>Alternatively, a AWS <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html">Amazon Federated Sign-On URL</a> can also be generated:</p>

<pre><code>curl https://&lt;API-GATEWAY&gt;/dev/api/login?role=&lt;ROLE-ARN&gt; \
-H &quot;Authorization: bearer $(token-cli token get aws-credentials-issuer)&quot;
</code></pre>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_17.png" alt="" /></center></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2017/02/26/scala-and-aws-managed-elasticsearch/">Scala and AWS managed ElasticSearch</a>
      </h1>
      <span class="post-date">Feb 26, 2017 &middot; 2 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/programming">programming</a>
      </span>
      
      

<p><a href="https://aws.amazon.com/">AWS</a> offer a <a href="https://aws.amazon.com/elasticsearch-service/">managed ElasticSearch service</a>. It exposes an HTTP endpoint for interacting with ElasticSearch and requires authentication via <a href="https://aws.amazon.com/documentation/iam/">AWS Identity Access Management</a>.</p>

<p><a href="https://github.com/sksamuel/elastic4s">Elastic4s</a> offers a neat DSL and Scala client for ElasticSearch. This post details how to use it with <a href="https://aws.amazon.com/">AWS&rsquo;s</a> <a href="https://aws.amazon.com/elasticsearch-service/">managed ElasticSearch service</a>.</p>

<h2 id="creating-a-request-signer">Creating a request signer</h2>

<p>Using the <a href="https://github.com/inreachventures/aws-signing-request-interceptor">aws-signing-request-interceptor</a> library its easy to create an <a href="https://hc.apache.org/httpcomponents-core-ga/httpcore/apidocs/org/apache/http/HttpRequestInterceptor.html">HttpRequestInterceptor</a> which can be later added to the HttpClient used by <a href="https://github.com/sksamuel/elastic4s">Elastic4s</a> for making the calls to ElasticSearch</p>

<pre><code class="language-scala">private def createAwsSigner(config: Config): AWSSigner = {
  import com.gilt.gfc.guava.GuavaConversions._

  val awsCredentialsProvider = new DefaultAWSCredentialsProviderChain
  val service = config.getString(&quot;service&quot;)
  val region = config.getString(&quot;region&quot;)
  val clock: Supplier[LocalDateTime] = () =&gt; LocalDateTime.now(ZoneId.of(&quot;UTC&quot;))
  new AWSSigner(awsCredentialsProvider, region, service, clock)
}
</code></pre>

<h2 id="creating-an-http-client-and-intercepting-the-requests">Creating an HTTP Client and intercepting the requests</h2>

<p>The ElasticSearch <a href="https://github.com/elastic/elasticsearch/blob/master/client/rest/src/main/java/org/elasticsearch/client/RestClientBuilder.java#L230">RestClientBuilder</a> allows for registering a callback to modify the customise the <a href="http://hc.apache.org/httpcomponents-asyncclient-dev/httpasyncclient/apidocs/org/apache/http/impl/nio/client/HttpAsyncClientBuilder.html#addInterceptorLast(org.apache.http.HttpResponseInterceptor)">HttpAsyncClientBuilder</a> enabling registering the interceptor to sign the requests.</p>

<p>The callback can be created by implementing the HttpClientConfigCallback interface as follows:</p>

<pre><code class="language-scala">private val esConfig = config.getConfig(&quot;elasticsearch&quot;)

private class AWSSignerInteceptor extends HttpClientConfigCallback {
  override def customizeHttpClient(httpClientBuilder: HttpAsyncClientBuilder): HttpAsyncClientBuilder = {
    httpClientBuilder.addInterceptorLast(new AWSSigningRequestInterceptor(createAwsSigner(esConfig)))
  }
}
</code></pre>

<p>Finally, an <a href="https://github.com/sksamuel/elastic4s">Elastic4s</a> client can be created with the interceptor registered:</p>

<pre><code class="language-scala">private def createEsHttpClient(config: Config): HttpClient = {
  val hosts = ElasticsearchClientUri(config.getString(&quot;uri&quot;)).hosts.map {
    case (host, port) =&gt;
      new HttpHost(host, port, &quot;http&quot;)
  }

  log.info(s&quot;Creating HTTP client on ${hosts.mkString(&quot;,&quot;)}&quot;)

  val client = RestClient.builder(hosts: _*)
    .setHttpClientConfigCallback(new AWSSignerInteceptor)
    .build()
  HttpClient.fromRestClient(client)
}
</code></pre>

<p>Full Example on <a href="https://github.com/imduffy15/scala-aws-hosted-es/blob/master/src/main/scala/Run.scala">GitHub</a></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2016/11/27/azure-bug-bounty-root-to-storage-account-administrator/">Azure bug bounty Root to storage account administrator</a>
      </h1>
      <span class="post-date">Nov 27, 2016 &middot; 2 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/cloud">Cloud</a><a class="label" href="http://ianduffy.ie/categories/security">Security</a><a class="label" href="http://ianduffy.ie/categories/azure">Azure</a>
      </span>
      
      <p>In my previous blog post <a href="http://ianduffy.ie/blog/2016/11/26/azure-bug-bounty-pwning-red-hat-enterprise-linux/">Azure bug bounty Pwning Red Hat Enterprise Linux</a> I detailed how it was possible to get administrative access to the Red Hat Update Infrastructure consumed by <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> virtual machines booted from the <a href="https://azure.microsoft.com">Microsoft Azure</a> Marketplace image. In theory, if exploited one could have gained root access to all virtual machines consuming the repositories by releasing an updated version of a common package and waiting for virtual machines to execute <code>yum update</code>.</p>

<p>As an attacker, this would have granted access to every piece of data on the compromised virtual machines. Sadly, the attack vector is actually much more widespread than this. Given some poor implementation within the mandatory <a href="https://azure.microsoft.com">Microsoft Azure</a> Linux Agent (WaLinuxAgent) one is able to obtain the administrator API keys to the storage account used by the virtual machine for debug log shipping purposes, at the time of research this storage account defaulted to one shared by multiple virtual machines.</p>

<p>At the time of research, the <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> image available on the <a href="https://azure.microsoft.com">Microsoft Azure</a> Marketplace came with WaLinuxAgent 2.0.16. When a virtual machine was created with the &ldquo;Linux diagnostic extension&rdquo; enabled the API key for access to the specified storage account was written to <code>/var/lib/waagent/Microsoft.OSTCExtensions.LinuxDiagnostic-2.3.9007/xmlCfg.xml</code>.</p>

<p>Once acquired one can simply use the <a href="https://github.com/Azure/azure-xplat-cli">Azure Xplat-CLI</a> to interact the storage account:</p>

<pre><code>export AZURE_STORAGE_ACCOUNT=&quot;storage_account_name_as_per_xmlcfg&quot;
export AZURE_STORAGE_ACCESS_KEY=&quot;storage_account_access_key_as_per_xmlcfg&quot;
azure storage container list # acquire some container name
azure storage blob list # provide the container name
# Copy, download, upload, delete any blobs available across any containers you can access.
</code></pre>

<p>If the storage account was used by multiple virtual machines there is potential to download their virtual hard disks.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2016/11/26/azure-bug-bounty-pwning-red-hat-enterprise-linux/">Azure bug bounty Pwning Red Hat Enterprise Linux</a>
      </h1>
      <span class="post-date">Nov 26, 2016 &middot; 5 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/cloud">Cloud</a><a class="label" href="http://ianduffy.ie/categories/security">Security</a><a class="label" href="http://ianduffy.ie/categories/azure">Azure</a>
      </span>
      
      

<p><em>TL;DR Acquired administrator level access to all of the <a href="https://azure.microsoft.com">Microsoft Azure</a> managed <a href="https://access.redhat.com/documentation/en/red-hat-update-infrastructure/3.0.beta.1/paged/system-administrator-guide/chapter-1-about-red-hat-update-infrastructure">Red Hat Update Infrastructure</a> that supplies all the packages for all <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> instances booted from the Azure marketplace.</em></p>

<p>I was tasked with creating a machine image of <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> that was compliant to the <a href="https://www.stigviewer.com/stig/red_hat_enterprise_linux_6/">Security Technical Implementation guide defined by the Department of Defense</a>.</p>

<p>This machine image was to be used for both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a>. Both of which offer marketplace images which had a metered billing pricing model[1][2]. Ideally, I wanted my custom image to be billed under the same mechanism, as such the virtual machines would be able to consume software updates from a local <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> repository owned and managed by the cloud provider.</p>

<p>Both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a> utilise a deployment of <a href="https://access.redhat.com/documentation/en/red-hat-update-infrastructure/3.0.beta.1/paged/system-administrator-guide/chapter-1-about-red-hat-update-infrastructure">Red Hat Update Infrastructure</a> for supplying this functionality.</p>

<p>This setup requires two main parts:</p>

<h3 id="red-hat-update-appliance">Red Hat Update Appliance</h3>

<p>There is only one Red Hat Update Appliance per <a href="https://access.redhat.com/documentation/en/red-hat-update-infrastructure/3.0.beta.1/paged/system-administrator-guide/chapter-1-about-red-hat-update-infrastructure">Red Hat Update Infrastructure</a> installation, however, both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a> create one per region.</p>

<p>The Red Hat Update Appliance is responsible for:</p>

<ul>
<li>Downloading new packages from the Red Hat CDN. It is the only component that requires a connection back to Red Hat.</li>
<li>Copying new packages to each content delivery server</li>
<li>Supplying a <a href="http://pulpproject.org/">REST API and command line interface for management of software repositories</a></li>
</ul>

<p><strong>The Red Hat Update Appliance does not need to be exposed to the repository clients.</strong></p>

<h3 id="content-delivery-server">Content Delivery server</h3>

<p>The content delivery server(s) provide the yum repositories that clients connect to for updated packages.</p>

<h2 id="achieving-metered-billing">Achieving metered billing</h2>

<p>Both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a> use SSL certifications for authentication against the repositories.</p>

<p>However, these are the same SSL certificates for every instance.</p>

<p>On <a href="https://aws.amazon.com/">Amazon Web Services</a> having the SSL certificates is not enough, you must have booted your instance from an AMI that had an associated billing code. It is this billing code that ensures you pay the extra premium for running <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a>.</p>

<p>On Azure it remains undefined how they manage to track billing. At the time of research, it was possible to copy the SSL certificates from one instance to another and successfully authenticate. Additionally, if you duplicated a <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> virtual hard disk and created a new instance from it all billing association seemed to be lost but repository access was still available.</p>

<h2 id="where-azure-failed">Where Azure Failed</h2>

<p>On Azure to setup repository connectivity, they provide an RPM with the necessary configuration. In the older version of their agent, it is responsible for this task [3].  The installation script it references comes from the following <a href="http://rhuirpm.blob.core.windows.net/script/rhui.tar.gz">archive</a>. If you expand this archive you will find the client configuration for each region.</p>

<p>By browsing the metadata of the RPMs we can discover some interesting information:</p>

<pre><code class="language-bash">$ rpm -qip RHEL6-2.0-1.noarch.rpm
Name        : RHEL6                        Relocations: (not relocatable)
Version     : 2.0                               Vendor: (none)
Release     : 1                             Build Date: Sun 14 Feb 2016 06:40:54 AM UTC
Install Date: (not installed)               Build Host: westeurope-rhua.cloudapp.net
Group       : Applications/Internet         Source RPM: RHEL6-2.0-1.src.rpm
Size        : 20833                            License: GPLv2
Signature   : (none)
URL         : http://redhat.com
Summary     : Custom configuration for a cloud client instance
Description :
Configurations for a client to connect to the RHUI infrastructure
</code></pre>

<p>As you can see, the build host enables us to discover all of the Red Hat Update Appliances:</p>

<pre><code class="language-bash">$ host westeurope-rhua.cloudapp.net
westeurope-rhua.cloudapp.net has address 104.40.209.83

$ host eastus2-rhua.cloudapp.net
eastus2-rhua.cloudapp.net has address 13.68.20.161

$ host southcentralus-rhua.cloudapp.net
southcentralus-rhua.cloudapp.net has address 23.101.178.51

$ host southeastasia-rhua.cloudapp.net
southeastasia-rhua.cloudapp.net has address 137.116.129.134
</code></pre>

<p>At the time of research, all of servers were exposing their REST APIs over HTTPs.</p>

<p>The URL to the archive containing these RPMs was discovered a package labeled PrepareRHUI on available on any <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> Box running on <a href="https://azure.microsoft.com">Microsoft Azure</a>.</p>

<pre><code class="language-bash">$ yumdownloader PrepareRHUI
$ rpm -qip PrepareRHUI-1.0.0-1.noarch.rpm
Name        : PrepareRHUI                  Relocations: (not relocatable)
Version     : 1.0.0                             Vendor: Microsoft Corporation
Release     : 1                             Build Date: Mon 16 Nov 2015 06:13:21 AM UTC
Install Date: (not installed)               Build Host: rhui-monitor.cloudapp.net
Group       : Unspecified                   Source RPM: PrepareRHUI-1.0.0-1.src.rpm
Size        : 770                              License: GPL
Signature   : (none)
Packager    : Microsoft Corporation &lt;xiazhang@microsoft.com&gt;
Summary     : Prepare RHUI installation for Redhat client
Description :
PrepareRHUI is used to prepare RHUI installation for before making a Redhat image.
</code></pre>

<p>The build host is interesting <code>rhui-monitor.cloudapp.net</code>, at the time of research running a port scan revealed an application running on port 8080.</p>

<p><img src="/monitor.png" alt="Microsoft Azure RHUI Monitoring tool" /></p>

<p>Despite the application requiring username and password based authentication, It was possible to execute a run of their &ldquo;backend log collector&rdquo; on a specified content delivery server. When the collector service completed the application supplied URLs to archives which contain multiple logs and configuration files from the servers.</p>

<p>Included within these archives was an SSL certificate that would grant full administrative access to the Red Hat Update Appliances [4].</p>

<p><img src="/directory.png" alt="Pulp admin keys for Microsoft Azure's RHUI" /></p>

<p>At the time of research all <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> virtual machines booted from the Azure Marketplace image had the following additional repository configured:</p>

<pre><code>[rhui-PA]
name=Packages for Azure
mirrorlist=https://eastus2-cds1.cloudapp.net/pulp/mirror/PA
enabled=1
gpgcheck=0
sslverify=1
sslcacert=/etc/pki/rhui/ca.crt
</code></pre>

<p>Given no gpgcheck is enabled, with full administrative access to the <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> Appliance REST API one could have uploaded packages that would be acquired by client virtual machines on their next yum update.</p>

<p>The issue was reported in accordance to the <a href="https://technet.microsoft.com/en-us/library/dn800983.aspx">Microsoft Online Services Bug Bounty terms</a>. Microsoft agreed it was a vulnerability in their systems. Immediate action was taken to prevent public access to <code>rhui-monitor.cloudapp.net</code>. Additionally, they eventually prevented public access to the Red Hat Update Appliances and they claim to have rotated all secrets.</p>

<p>[1] <a href="https://azure.microsoft.com/en-in/pricing/details/virtual-machines/red-hat/">https://azure.microsoft.com/en-in/pricing/details/virtual-machines/red-hat/</a></p>

<p>[2] <a href="https://aws.amazon.com/partners/redhat/">https://aws.amazon.com/partners/redhat/</a></p>

<p>[3] <a href="https://github.com/Azure/azure-linux-extensions/blob/master/Common/WALinuxAgent-2.0.16/waagent#L2891">https://github.com/Azure/azure-linux-extensions/blob/master/Common/WALinuxAgent-2.0.16/waagent#L2891</a></p>

<p>[4] <a href="https://fedorahosted.org/pulp/wiki/Certificates">https://fedorahosted.org/pulp/wiki/Certificates</a></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2016/06/28/hello-world/">Hello World</a>
      </h1>
      <span class="post-date">Jun 28, 2016 &middot; 1 minute read
      
      <br/>
      
      </span>
      
      <pre><code class="language-ruby">resource &quot;null_resource&quot; &quot;hello_world&quot; {
    provisioner &quot;local-exec&quot; {
        command = &quot;echo 'Hello World'&quot;
    }
}
</code></pre>

<p>Interested in automation and the HashiCorp suite of tools? If so you&rsquo;ll love this blog.
Through different posts we will explore lots different automation tasks utilising both
public and private cloud with the HashiCorp toolset.</p>

<p>Thanks,
Ian.</p>

      
    </div>
    
    

  </div>
</div>


<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>

