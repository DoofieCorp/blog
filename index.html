<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.55.6" />
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>Ian Duffy | Rants of a software engineer</title>

  
  <link rel="stylesheet" href="/css/poole.css">
  <link rel="stylesheet" href="/css/hyde.css">
  <link rel="stylesheet" href="/css/poole-overrides.css">
  <link rel="stylesheet" href="/css/hyde-overrides.css">
  <link rel="stylesheet" href="/css/hyde-x.css">
  <link rel="stylesheet" href="/css/highlight/sunburst.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
  <link href="/favicon.png" rel="icon">

  
  
  
  <link href="http://ianduffy.ie/index.xml" rel="alternate" type="application/rss+xml" title="Ian Duffy &middot; Ian Duffy" />

  <meta name="description" content="Your default page description">
  <meta name="keywords" content="your,default,page,keywords">
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-33060296-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      
      <h1>Ian Duffy</h1>
      <p class="lead">Rants of a software engineer</p>
    </div>

    <ul class="sidebar-nav">
      
    </ul>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <a href="http://github.com/imduffy15"><i class="fa fa-github-square fa-3x"></i></a>
      
      
      <a href="https://ie.linkedin.com/in/imduffy15"><i class="fa fa-linkedin-square fa-3x"></i></a>
      
      
      
      <a href="https://www.youtube.com/channel/UCoGs2iiOIGrfXofp-3g-Qqg"><i class="fa fa-youtube-square fa-3x"></i></a>
      
      </li>
    </ul>

    
  </div>
</div>


<div class="content container">
  <div class="posts">
    
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2019/09/11/6-months-of-working-remotely-at-scrapinghub.com/">6 months of working remotely at scrapinghub.com</a>
      </h1>
      <span class="post-date">Sep 11, 2019 &middot; 13 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/lifestyle">lifestyle</a>
      </span>
      
      

<p><a href="https://scrapinghub.com/">ScrapingHub</a> is a distributed company that builds tooling and a platform to extract data from the web. The company was incorporated in <a href="https://www.google.com/maps/place/Cork/@51.8959999,-8.4980691,13z">Ireland</a> in 2010, from day 1 it was a distributed team with many staff in Uruguay and other parts of the world. Since then the company has grown and now has a team of 180 people located all around the world, last year the companies revenue was 12 million. I joined in March 2019, it was my first time to work remotely. Below I reflect on my past 6 months, the remote experience and some of the things I’ve been working on.</p>

<h2 id="what-is-scrapinghub-https-scrapinghub-com">What is <a href="https://scrapinghub.com">Scrapinghub</a>?</h2>

<p>The mission is “to provide our customers the data they need to innovate and grow their businesses.&rdquo; The founders of the company are the creators of <a href="https://scrapy.org/">Scrapy</a>, a very popular open source and collaborative framework for extracting data from the web. ScrapingHub started as a <a href="https://scrapinghub.com/scrapy-cloud">platform for running Scrapy spiders</a>. It has since grown to include three more products:</p>

<ul>
<li><a href="https://scrapinghub.com/crawlera">Crawlera</a>: A specially designed proxy for web scraping to ensure you can crawl quickly and reliably</li>
<li><a href="https://scrapinghub.com/splash">Splash</a>: A headless browser to enable customers to extract data from JavaScript websites</li>
<li><a href="https://scrapinghub.com/autoextract">AutoExtract</a>: delivers next-generation web scraping capabilities backed by an <a href="https://blog.scrapinghub.com/artificial-intelligence-data-extraction-api">AI-enabled data extraction engine</a>. This enables customers to crawl many websites without needing to write custom CSS and XPath selectors for each one.</li>
</ul>

<h2 id="what-is-your-role">What is your role?</h2>

<p>I am a DevOps engineer on the <a href="https://scrapinghub.com/autoextract">AutoExtract</a> product. I work closely with two backend developers to deliver an API that enables customers to access our <a href="https://blog.scrapinghub.com/artificial-intelligence-data-extraction-api">AI-enabled data extraction engine</a>. I’m dedicated to providing infrastructure services for the team, this includes things like compute, deployment mechanisms, monitoring, alerting and logging, and so on.</p>

<h2 id="what-is-your-background-and-what-brought-you-to-scrapinghub">What is your background and what brought you to ScrapingHub?</h2>

<p>I graduated from <a href="https://www.dcu.ie/courses/undergraduate/computing/computer-applications.shtml">Computer Applications at Dublin City University</a> 5 years ago. During my time in college, I had a great interest in infrastructure and automation. Upon graduating, I worked as a Cloud Engineer in a bank for a year before joining an ecommerce company under the title of Software Engineer.</p>

<p>At this company, I worked on a team that acquired data from the web. However, we never neatly solved the problem, our solution would never scale. This was due to the initial work to create XPaths and CSS selectors along with the ongoing maintenance and monitoring to ensure the sites haven’t changed and these are now broken. During my time in this role, I had heard of <a href="https://scrapinghub.com">ScrapingHub</a> as they are one of the leaders in the web scraping space. It was always a company I was curious to know more about and it was unusual to hear of an Irish company that is distributed.</p>

<p>After this, I was employee number one an Irish Start-Up, again with the title of Software Engineer. As you can imagine in a Start-Up environment there is lots of work to do and the role was a diverse one that included many different aspects.</p>

<p>My journey to ScrapingHub began with a call from a <a href="https://www.linkedin.com/in/garrybarcoe/">recruiter</a> in <a href="https://www.stelfox.com/">Stelfox</a> who told me about the role. He had my attention at the word <a href="https://scrapinghub.com/">ScrapingHub</a>, I knew immediately this was a chance to experience remote working for the first time. During the conversation, my interest only began to grow. The recruiter described the <a href="https://scrapinghub.com/autoextract">AutoExtract</a> product to me and I was curious to know more - they’ve figured a way to do web scraping without the need to write XPaths and CSS selectors, a problem I previously faced.</p>

<h2 id="what-was-the-interview-and-acceptance-process-like">What was the interview and acceptance process like?</h2>

<p>As ScrapingHub is a distributed company all interviews are done remotely, you do not meet a Shubber (someone who works for ScrapingHub) in person at any stage. For me, the interview process consisted of three interviews each lasting no more than 30 minutes.</p>

<p>My first interview was with a <a href="https://www.linkedin.com/in/jessicalouisequinn/">ScrapingHub recruiter</a>. I got the impression that the interview focused on whether or not you and ScrapingHub are a good fit for each other. It&rsquo;s a time for them to ask you questions to see if you align with their <a href="https://scrapinghub.com/jobs">company values</a> and for you to ask questions to see if its the right role for you.</p>

<p>My second interview was the hardest out of the three. It was with a member of the infrastructure team, it was very technical focused and enjoyable experience. For me, the interview was a stream of questions covering different key areas of the role. The questions started easy and then increased in difficulty. For example, the containers section started with “What flag do you pass to docker to expose a port?” and ended on “Describe to the best of your knowledge how containers work at a kernel level?”. When I failed to have a good answer for a question the interviewer was nice about it and discussed it in more detail, I liked this and I walked away from the interview having learned something.</p>

<p>The final interview was with the head of engineering. I believe this interview was a mix of both culture fit and technical ability. It took the form of a friendly conversation based on resume items and previous work along with a few technical questions thrown in along the way.</p>

<p>A couple of days later I heard back from the Stelfox recruiter to inform me that my interview had been successful. From this point on out, it was a standard procedure - receive a contract, provide personal details, etc.</p>

<h2 id="setting-myself-up-for-success">Setting myself up for success</h2>

<p><center><img src="/images/6-months-of-working-remote-with-scrapinghub/desk.png" alt="" /></center></p>

<p>As I’m based in Ireland <a href="https://scrapinghub.com">ScrapingHub</a> provided me with a laptop that was shipped to my address before I was due to start. I was excited about the job and remote aspect of it. I believed having a good working area was going to be key to my success.</p>

<p>I spent a couple of days trawling second hand sites and researching office furniture and equipment. At the time I was living in a small 1 bedroom apartment in <a href="https://www.google.com/maps/place/Dublin/@53.3242381,-6.3857842,11z">Dublin, Ireland</a> and space was limited. I focused on what I felt were the key elements to making a reasonable space where I could work contently. I ended up with the following:</p>

<ul>
<li>Monitor: <a href="https://www.amazon.com/Dell-U3419w-Ultrasharp-34-Inch-3440x1440/dp/B07HB3ZX9F">Dell U3419w Ultrasharp 34-Inch</a></li>
<li>Mouse: <a href="https://www.amazon.co.uk/gp/product/B0761YL588/ref=ppx_yo_dt_b_asin_title_o06_s00?ie=UTF8&amp;psc=1">Logitech MX Master</a></li>
<li>Keyboard: <a href="https://www.daskeyboard.com/model-s-professional-for-mac/">Das Keyboard Model S Professional</a></li>
<li>Webcam:<a href="https://www.amazon.co.uk/gp/product/B006A2Q81M/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1"> Logitech C920</a></li>
<li>Chair: <a href="https://www.adverts.ie/for-sale/q_herman+miller+aeron/">Herman Miller Aeron</a></li>
<li>Mouse mat: <a href="https://www.amazon.co.uk/gp/product/B00EG7WB8C/ref=ppx_yo_dt_b_asin_title_o06_s00?ie=UTF8&amp;psc=1">Perixx DX-1000XXL Gaming Mouse Mat</a></li>
<li>Headphones/Microphone: <a href="https://www.bose.ie/en_ie/products/headphones/over_ear_headphones/quietcomfort-35-wireless-ii.html">Bose QuietComfort 35</a></li>
</ul>

<p>I’ve since made use of the remote flexibility and moved away from Dublin, Ireland to a bigger property in Cork, Ireland. In doing so I’ve improved my workspace by having a dedicated office with a <a href="https://www.adverts.ie/office-desks/sit-stand-ergonomic-desk-electrically-operated/17841025">standing desk</a> and <a href="https://www.amazon.co.uk/D-Line-Extension-Electrical-Management-Electrically-Safe/dp/B0076XD7HC/ref=pd_rhf_ee_s_rp_c_0_5/260-0626086-0317246?_encoding=UTF8&amp;pd_rd_i=B0076XD7HC&amp;pd_rd_r=8ae93a0c-f956-41b4-a45e-1db92eb9a8d1&amp;pd_rd_w=qCqzf&amp;pd_rd_wg=bqXL3&amp;pf_rd_p=25145bb1-83c2-4724-9d46-08e5c49d4552&amp;pf_rd_r=NNGJ8NNH98XFQHR2876T&amp;psc=1&amp;refRID=NNGJ8NNH98XFQHR2876T">many</a>, <a href="https://www.amazon.co.uk/gp/product/B07DQLHVJX/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1">many</a>, <a href="https://www.amazon.co.uk/gp/product/B0725ZZBLV/ref=ppx_yo_dt_b_asin_image_o01_s00?ie=UTF8&amp;psc=1">many</a>, <a href="https://www.amazon.co.uk/gp/product/B00347A876/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1">many</a> <a href="https://www.amazon.co.uk/gp/product/B00FYRVY2M/ref=ppx_yo_dt_b_asin_title_o02_s00?ie=UTF8&amp;psc=1">cable</a> <a href="https://www.amazon.co.uk/gp/product/B078WMCWDD/ref=ppx_yo_dt_b_asin_title_o02_s00?ie=UTF8&amp;psc=1">management</a> aids.</p>

<h2 id="the-first-week">The first week</h2>

<p><center><img src="/images/6-months-of-working-remote-with-scrapinghub/slack.png" alt="" /></center></p>

<p>Three days before I was due to start I received an email containing my staff credentials. This introduced me to some of the tools used for management and communication:</p>

<ul>
<li>JIRA - For managing work and tracking time.</li>
<li>Confluence An internal knowledge base for the team to collaboratively maintain.</li>
<li>GSuite - Email, calendar, and meetings.</li>
<li>Github - Git repositories</li>
<li>BambooHR - HR System</li>
</ul>

<p>Before officially starting I signed into Slack and received a warm welcome from all my future co-workers. I also signed into my GSuite account where my calendar revealed a hint of what the first few days were going to be like.</p>

<p>One of my major concerns early on was keeping to a routine. To help with this I decided to continue waking up at the same time as I did in previous jobs. I wanted to use the time I saved by having no commute to complete any tasks that involved leaving the house. For example, on day 1 I did the grocery shopping for the week.</p>

<p><center><img src="/images/6-months-of-working-remote-with-scrapinghub/day1.png" alt="" /></center>
<center><img src="/images/6-months-of-working-remote-with-scrapinghub/generic-day.png" alt="" /></center></p>

<p>With the groceries unpacked and put away, it was time to get to work. My first morning started at 9.30am, it consisted of HR introducing me to my manager. He told me a little bit about the team and followed up with inviting me to team-specific slack channels and scheduling introductions with the team.</p>

<p><center><img src="/images/6-months-of-working-remote-with-scrapinghub/calendar.png" alt="" /></center></p>

<p>After this at 10 am was a general company HR onboarding session. New hires are brought on in batches so I wasn’t alone on these calls, for me there were 3 other new hires on the call. Immediately after this, we had a 1:1 call with HR to discuss our benefits (pension, shares, holidays, health insurance, etc.).</p>

<p>Before finishing up for lunch we had an IT onboarding session. This meeting was to ensure we could access all the necessary systems and that we knew different company policies - How do I change my password? What do I do if I forgot my password? How do I encrypt my disk? What do I do in the event a device is stolen? And so on.</p>

<p>ScrapingHub is very focused on wellbeing. For March they ran an event which focused on this. As you can see from my calendar, the next meeting was “Stay sane while working remote” with another later in the week of “Mindful Meditation” both of these events were March specials. As a newbie it was interesting to experience these, it gave me a glimpse of how ScrapingHub encourages camaraderie and enables people to get to know each other outside of work topics. Outside of March this space for general chit-chat is maintained, each week there’s a standing watercooler meeting for the different time zones for people to get together and chat about absolutely anything. Additionally, each month we have “Shub talks” where people show something they’ve been working on or share some piece of knowledge - I’m pleased to say within my 6 months at the company I have had the opportunity to speak at one.</p>

<p>Opensource has been a big part of ScrapingHub since day 1, this is demonstrated by Scrapy being launched as an opensource project by the company back in 2009. My final meeting of the day showed how opensource is still a very big part of ScrapingHub. Shubbers are encouraged to contribute to opensource as much as possible, we are given 5 hours per week to engage in anything opensource related. Today ScrapingHub has a handful of opensource projects, check out some of the more popular ones: <a href="https://github.com/scrapinghub/splash">Splash</a>, <a href="https://github.com/TeamHG-Memex/eli5">ELI5</a>, <a href="https://github.com/TeamHG-Memex/deep-deep">deep-deep</a>, and <a href="https://github.com/scrapinghub/spidermon">SpiderMon</a>. Additionally, ScrapingHub supports the opensource ecosystem by part-taking in <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a>.</p>

<p>Day two continued with more meetings. The morning section was introductions to the team and a system overview. Following these, the newbie group had an introduction to the founders of the company. By the end of the day, I had been assigned a ticket in JIRA and was working away on it.</p>

<h2 id="what-have-you-been-working-on">What have you been working on?</h2>

<p>Up until now, infrastructure at ScrapingHub ran on dedicated servers rented from Hetzner. Teams package their applications up as Docker containers and uses a ChatOps deployment mechanism to run them on Mesos. Additional services such as a database or kafka require input from the infrastructure team.</p>

<p>Previously to me joining the company, a decision was made to move AutoExtract onto Google Cloud Platform with their managed Kubernetes offering. The idea behind this was to be able to enable us to quickly increase compute to meet customer demand.</p>

<p>My main task was to make this happen. Thankfully due to the adoption of Docker and Mesos within the company this challenge I had a bit of a head start.</p>

<p>AutoExtract is build-up of multiple components that communicate together via Kafka. It is exposed to customers via an HTTP API. Breaking this down I established what was required to get the system running, this went as follows:</p>

<ul>
<li>A mechanism to deploy and destroy a single component - <a href="https://helm.sh/">Helm</a> fits this use case nicely. I created a helm chart for each of the components. The chart is stored, versioned and released with the component’s source code. I extended the pre-existing ChatOp deployment mechanism to support communicating with helm.</li>
<li>HTTP/HTTPS connectivity to pods - <a href="https://github.com/kubernetes/ingress-nginx">Ingress Nginx</a> solved this problem nicely. I paired it with <a href="https://github.com/jetstack/cert-manager">cert manager</a> to automatically provision SSL certificates from letsencrypt and <a href="https://github.com/kubernetes-incubator/external-dns">External DNS</a> to automatically create DNS records.</li>
<li>Monitoring of the application and cluster - Deploying <a href="https://prometheus.io/">Prometheus</a> provided a straightforward way to achieve this.</li>
<li>Authentication for internal applications - <a href="https://github.com/pusher/oauth2_proxy">OAuth2 Proxy</a> integrated nicely with ingress nginx to provide authentication against the company’s GSuite accounts.</li>
<li>Kafka - I made use of <a href="https://confluent.cloud/">Confluent.Cloud</a>’s managed kafka offering for this. Allowed us to get Kafka on a consumption pricing model and avoid an ongoing maintenance overhead.</li>
</ul>

<p>Today I’m happy to say that the AutoExtract product runs entirely on Google Cloud Platform.</p>

<h2 id="what-is-your-average-week-like">What is your average week like?</h2>

<p>Something that amazed me about working remotely is how much more productive I am. ScrapingHub enables me to have a large amount of autonomy over my work as well as my time. I am in control of how I wish to structure my day, this allows for lots of flexibility - nothing is stopping me taking an hour here and there throughout general working hours (9 - 5 pm) and replacing them with hours late into the evening.</p>

<p>My average week consists of a total of 3 mandatory meetings. Every Monday my manager does a 1:1 meeting with all members of the team, these last on average 15 minutes. On Tuesday we have a backend team sync up and on Thursday we have a full team sync up, each of these meetings last 1 hour and take a similar format to a daily standup with an allowance for more discussion where necessary.</p>

<p>For the rest of the week, I’m free to work away on whatever task has been assigned to me. Everyone is always available via Slack and when more in-depth communication is necessary ad-hoc video calls are started.</p>

<h2 id="frequently-asked-questions">Frequently asked questions?</h2>

<p><span style="text-decoration:underline;">Do you ever find remote work isolating or miss human contact?</span></p>

<p>No, the team I’m on is very cohesive. Everyone is very supportive and has a clear invested interest in the success of the product and thus the task you may be working on.</p>

<p>For times when I wish to get out of the house, ScrapingHub has hotdesk spaces in WeWork in Dublin and Republic of Work in Cork. The <a href="https://businessbanking.bankofireland.com/campaigns/bank-of-ireland-workbench/">Bank of Ireland Workbench spaces </a>or public libraries can be useful spots to work from. For the most part, I only make use of them when I have plans to meet a friend for lunch.</p>

<p><center><img src="/images/6-months-of-working-remote-with-scrapinghub/co-working-space.png" alt="" /></center></p>

<p>In addition to this, it&rsquo;s important to note that despite only being in the company for 6 months I was given the opportunity to meet all of my teammates in person along with the founders of the company and many members of the leadership team.</p>

<p><span style="text-decoration:underline;">Do you find a cost saving in working from home?</span></p>

<p>Yes, I no longer spend any money on commuting or purchasing lunch. On average I’ve worked this out at about a €80 saving per week. In addition to this, I’ve recently moved away from Dublin and reduced my rent cost significantly.</p>

<p><span style="text-decoration:underline;">Have you kept to your goal of wanting to keep a routine and doing something with the saved commuting time?</span></p>

<p>I would like to say yes, but sometimes extra sleep wins. I’m aiming to bring this back on track soon.</p>

<p><span style="text-decoration:underline;">What has been your biggest personal challenge with switching to remote work?</span></p>

<p>Definitely communication, specifically video calls. I found seeing my face on screen for the first few days very odd. These days I’ve become comfortable with that and just struggle with knowing the correct time to speak or interrupt to add to something while someone else is talking.</p>

<p><span style="text-decoration:underline;">What do you most enjoy about working remote?</span></p>

<p><img src="/images/6-months-of-working-remote-with-scrapinghub/rain.png" alt="" /></p>

<p>I really like the hassle free element of it, I’m no longer setting alarm clocks, checking public transport times, wondering if I need to bring an umbrella today, and so on. Starting and finishing work is as simple as entering and exiting a room in my house.</p>

<p><span style="text-decoration:underline;">How does knowledge sharing within your team occur?</span></p>

<p>For me, this is mainly done through Slack, the team are very supportive and willing to help as much as they can. I try to return this as much has I can and believe I have mostly been successful given the teams ability to adopt kubernetes.</p>

<p>If you’ve got any other questions feel free to <a href="mailto:ian@ianduffy.ie">reach out</a>.</p>

<h2 id="interested-in-going-remote-with-scrapinghub">Interested in going remote with ScrapingHub?</h2>

<p>I’ve enjoyed my time at ScrapingHub so far and I’m sure you might too. If you enjoyed my experience above and would like to try it out yourself take a look at the <a href="https://scrapinghub.com/jobs">jobs page</a> or research out to a <a href="mailto:scrapinghub@jobs.workablemail.com">ScrapingHub Recruiter.</a></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2019/02/22/exporting-confluent-cloud-metrics-to-prometheus/">Exporting Confluent Cloud Metrics to Prometheus</a>
      </h1>
      <span class="post-date">Feb 22, 2019 &middot; 1 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/cloud">Cloud</a><a class="label" href="http://ianduffy.ie/categories/docker">Docker</a>
      </span>
      
      <p>At Kafka Summit this year, Confluent announced consumption based billing for their <a href="https://confluent.cloud/">Kafka Cloud</a> offering, making it the cheapest and easiest way to get a Kafka Cluster. However, due to the Kafka cluster being multi-tenanted it comes with some restrictions, ZooKeeper is not exposed and the <code>__consumer_offsets</code> topic is restricted, this means popular tools like <a href="https://github.com/yahoo/kafka-manager">Kafka Manager</a> and <a href="https://github.com/braedon/prometheus-kafka-consumer-group-exporter">Prometheus Kafka Consumer Group Exporter</a> won&rsquo;t work.</p>

<p><a href="https://github.com/danielqsj/kafka_exporter">kafka_exporter</a> comes as a nice alternative as it uses the Kafka Admin Client to access the metrics. However, due to the authentication process required by <a href="https://confluent.cloud/">Confluent Cloud</a> it doesn&rsquo;t work as is.</p>

<p>By forking <a href="https://github.com/imduffy15/kafka_exporter">kafka_exporter</a> and upgrading the Kafka client one can get a successful output.</p>

<p>You can try out my build as follows:</p>

<pre><code>$ docker run -p 9308:9308 -it imduffy15/kafka_exporter \
--kafka.server=host.region.gcp.confluent.cloud:9092 \
--sasl.enabled \
--sasl.username=username \
--sasl.password=&quot;password&quot; \ 
--sasl.handshake \ 
--tls.insecure-skip-tls-verify \
--tls.enabled
</code></pre>

<p>and on querying <code>http://localhost:9308/metrics</code> all metrics documented <a href="https://github.com/imduffy15/kafka_exporter#topics">here</a> will be available. Prometheus can scrape this and alert and graph on the data.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2019/02/22/local-development-with-virtual-hosts-and-https/">Local development with virtual hosts and HTTPS</a>
      </h1>
      <span class="post-date">Feb 22, 2019 &middot; 4 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/development">Development</a><a class="label" href="http://ianduffy.ie/categories/web">Web</a><a class="label" href="http://ianduffy.ie/categories/docker">Docker</a>
      </span>
      
      

<p><center><img src="/images/local-development-with-virtual-hosts-and-https/diagram.png" alt="" /></center></p>

<p>When doing development locally it might be necessary to access the application(s) using a virtual host (vhost) and/or HTTPS. This post describes an approach to achieving this on OSX using Docker, which avoids creating a large mess on your computer.</p>

<h2 id="domain-name-systems-dns">Domain Name Systems (DNS)</h2>

<p>Domain Name Systems (DNS) are often referred to as the phonebook of the internet. People access information online through domain names, like ‘amazon.com’ or ‘rte.ie’. DNS is responsible for translating a domain name to an IP address.</p>

<p>When an HTTP resource is accessed using DNS, an additional header containing the domain name is provided to the HTTP server. The HTTP server uses this for routing the request to the correct vhost.</p>

<p>In order to have functioning vhosts, DNS is required. <a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">Dnsmasq</a> is a popular DNS server, it can be configured to resolve a set of specified domains to a specified IP address. Using docker a dnsmasq server can easily be created:</p>

<pre><code>$ docker run -it -p 53:53/udp --cap-add=NET_ADMIN andyshinn/dnsmasq:latest --log-queries --log-facility=- --address=/dev.ianduffy.ie/127.0.0.1
</code></pre>

<p>The provided command line arguments instruct dnsmasq to resolve all requests for dev.ianduffy.ie and *.dev.ianduffy.ie to 127.0.0.1. Using dig - a DNS lookup utility this configuration can be validated:</p>

<pre><code>$ dig @127.0.0.1 dev.ianduffy.ie +short
127.0.0.1
</code></pre>

<p>Additionally, all wildcards of dev.ianduffy.ie also resolve:</p>

<pre><code>$ dig @127.0.0.1 random-string.dev.ianduffy.ie +short
127.0.0.1
</code></pre>

<p>While this works, the OSX is not configured to use this DNS server for it’s lookups so attempting to resolve dev.ianduffy.ie in the browser will fail. It&rsquo;s possible to specify nameservers for a specific domain name, this can be achieved by creating a file within /etc/resolver with a filename that matches the domain.</p>

<p>For example, if /etc/resolver/dev.ianduffy.ie is created with the following contents:</p>

<pre><code>nameserver 127.0.0.1
</code></pre>

<p>Now all queries to dev.ianduffy.ie will be resolved by doing a lookup against the DNS server at 127.0.0.1.</p>

<h2 id="http">HTTP</h2>

<p>A HTTP server will be required for routing the requests based on a vhost and supplying HTTPS. Nginx is a good fit for this, even-more-so as <a href="https://www.linkedin.com/in/jason-wilder-94549/">Jason Wilder</a> from Microsoft has created a  <a href="https://github.com/jwilder/nginx-proxy">container image</a> that exposes the nginx reverse proxy functionality via environment variables.</p>

<p><a href="https://github.com/jwilder/nginx-proxy">nginx-proxy</a> can be started using the following:</p>

<pre><code>$ docker run -it -p 80:80 -v /var/run/docker.sock:/tmp/docker.sock:ro jwilder/nginx-proxy
</code></pre>

<p><a href="https://github.com/jwilder/nginx-proxy">nginx-proxy</a> will look for <code>VIRTUAL_HOST</code> environment variables on other docker containers and route to them accordingly. To demonstrate this, a container running <a href="https://httpbin.org">httpbin</a> which provides data for debugging HTTP requests can be created, with a VIRTUAL_HOST environment variable specified.</p>

<pre><code>$ docker run -e VIRTUAL_HOST=httpbin.dev.ianduffy.ie kennethreitz/httpbin
</code></pre>

<p>This service can now be accessed via httpbin.dev.ianduffy.ie. Alternatively, if you do not have the dnsmasq service from earlier running, the service can be accessed by passing the header &ldquo;host&rdquo; with value &ldquo;httpbin.dev.ianduffy.ie&rdquo;. This can be tested with an HTTP client like <a href="https://curl.haxx.se/">curl</a></p>

<pre><code>$ curl http://httpbin.dev.ianduffy.ie/headers
{
  &quot;headers&quot;: {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Connection&quot;: &quot;close&quot;,
    &quot;Host&quot;: &quot;httpbin.dev.ianduffy.ie&quot;,
    &quot;User-Agent&quot;: &quot;curl/7.54.0&quot;
  }
}
</code></pre>

<pre><code>$ curl -H &quot;host: httpbin.dev.ianduffy.ie&quot; http://127.0.0.1/headers
{
  &quot;headers&quot;: {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Connection&quot;: &quot;close&quot;,
    &quot;Host&quot;: &quot;httpbin.dev.ianduffy.ie&quot;,
    &quot;User-Agent&quot;: &quot;curl/7.54.0&quot;
  }
}
</code></pre>

<h2 id="https">HTTPS</h2>

<p>In some scenarios, HTTPS might be required. <a href="https://github.com/FiloSottile/mkcert">mkcert</a> provides locally trusted SSL certificates and automatically OSX, Linux, and Windows system stores along with Firefox, Chrome and Java.</p>

<p>With <a href="https://github.com/FiloSottile/mkcert">mkcert</a> installed, certificates can be generated with the following command:</p>

<pre><code>$ mkdir certs
$ mkcert -cert-file certs/dev.ianduffy.ie.crt -key-file certs/dev.ianduffy.ie.key -install dev.ianduffy.ie *.dev.ianduffy.ie
</code></pre>

<p>By mounting these certificates, as a volume on a container running nginx-proxy HTTPS will be enabled.</p>

<pre><code>$ docker run -it -p 80:80 -p 443:443 -v $(pwd)/certs:/etc/nginx/certs -v /var/run/docker.sock:/tmp/docker.sock:ro jwilder/nginx-proxy
</code></pre>

<p>Executing <code>curl</code> to <a href="https://httpbin.dev.ianduffy.ie">https://httpbin.dev.ianduffy.ie</a> will now respond successfully. Additionally, the <code>-v</code> flag can be specified to tell curl to be verbose and it will display information about the SSL certificate.</p>

<pre><code>$ curl -v https://httpbin.dev.ianduffy.ie/headers
</code></pre>

<h2 id="placing-a-vhost-in-front-of-a-local-application">Placing a vhost in front of a local application</h2>

<p>Most developers will want to run their application on the host and continue with the standard development workflow. To accommodate for this, a container that forwards traffic to a host port can be created.</p>

<p><a href="http://www.dest-unreach.org/socat/doc/README">socat</a> is perfect for this use case, it enables us to specify a source IP address and port and a destination IP address and port. In the example below, all traffic for test.dev.ianduffy.ie will be forwarded to the docker host on port 9000.</p>

<pre><code>$ docker run -it --expose 80 -e VIRTUAL_HOST=test.dev.ianduffy.ie alpine/socat tcp-listen:80,fork,reuseaddr tcp-connect:host.docker.internal:9000
</code></pre>

<h2 id="bring-it-all-together-with-docker-compose">Bring it all together with docker-compose</h2>

<p>All of the containers described above can be brought together in a single docker compose file. This provides ease of bringing the system up with a single command.</p>

<pre><code>nginx-proxy:
  container_name: nginx-proxy
  image: jwilder/nginx-proxy
  ports:
    - 80:80
    - 443:443
  volumes:
    - /var/run/docker.sock:/tmp/docker.sock:ro
    - ./certs:/etc/nginx/certs:ro
dnsmasq:
  container_name: dnsmasq
  image: andyshinn/dnsmasq:latest
  command: --log-queries --log-facility=- --address=/dev.ianduffy.ie/127.0.0.1
  ports:
    - 53:53
    - 53:53/udp
  cap_add:
    - NET_ADMIN
socat:
  image: alpine/socat
  command: tcp-listen:80,fork,reuseaddr tcp-connect:host.docker.internal:9000
  environment:
    - VIRTUAL_HOST=dev.ianduffy.ie
    - VIRTUAL_PORT=80
  expose:
    - 80
</code></pre>

<p>This enables VHosts and HTTPS for applications running on the host without creating too much of a mess as its all contained within Docker.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2018/12/15/managing-access-to-multiple-aws-accounts-with-openid/">Managing access to multiple AWS Accounts with OpenID</a>
      </h1>
      <span class="post-date">Dec 15, 2018 &middot; 6 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/cloud">Cloud</a><a class="label" href="http://ianduffy.ie/categories/security">Security</a><a class="label" href="http://ianduffy.ie/categories/aws">AWS</a><a class="label" href="http://ianduffy.ie/categories/serverless">Serverless</a>
      </span>
      
      

<p>Many organisations look towards a multiple account strategy with Amazon Web Services (AWS) to provide administrative isolation between workloads, limited visibility and discoverability of workloads, isolation to minimize blast radius, management of AWS limits and cost categorisation. However, this comes at a large complexity cost, specifically around Identity Access Management (IAM).</p>

<p>Starting off with a single AWS account, and using a handful of IAM users and groups for access management, is usually the norm. As an organisation grows they start to see a need for separate staging, production, and developer tooling accounts. Managing access to these can quickly become a mess. Do you create a unique IAM user in each account and provide your employees with the unique sign-on URL? Do you create a single IAM user for each employee and use <a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"><code>AssumeRole</code></a> to generate credentials and to enable jumping between accounts? How do employees use the AWS Application Programming Interface (API) or the Command Line Interface (CLI); are long-lived access keys generated? How is an employee&rsquo;s access revoked should they leave the organisation?</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_0.jpg" alt="" /></p>

<p>User per account approach</center></p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_1.jpg" alt="" /></p>

<p>All users in a single account</p>

<p>using STS AssumeRole to access other accounts</center></p>

<h1 id="design">Design</h1>

<h2 id="reuse-employees-existing-identities">Reuse employees existing identities</h2>

<p>In most organisations, an employee will already have an identity, normally used for accessing e-mail. These identities are normally stored in Active Directory (AD), Google Suite (GSuite) or Office 365. In an ideal world, these identities could be reused and would grant access to AWS. This means employees would only need to remember one set of credentials and their access could be revoked from a single place.</p>

<h2 id="expose-an-openid-compatible-interface-for-authentication">Expose an OpenID compatible interface for authentication</h2>

<p>OpenID provides applications with a way to verify a users identity using JSON Web Tokens (JWT). Additionally, it provides profile information about the end user such as first name, last name, email address, group membership, etc. This profile information can be used to store the AWS accounts and AWS roles the user has access to.</p>

<p>By placing an OpenID compatible interface on top an organisation&rsquo;s identity store users can easily generate JWTs which can be later used by services to authenticate them.</p>

<h2 id="trading-jwts-for-aws-api-credentials">Trading JWTs for AWS API Credentials</h2>

<p>In order to trade JWTs for AWS API Credentials, a service can be created that runs on AWS with a role that has access to AssumeRole.This service would be responsible for validating a users JWT, ensuring the JWT contains the requested role and executing STS AssumeRole to generate the AWS API Credentials.</p>

<p>Additionally, the service would also generate an <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html">Amazon Federated Sign-On URL</a> which would enable users to access the AWS Web Console using their JWT.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_2.png" alt="" /></center></p>

<h1 id="example-implementation">Example Implementation</h1>

<p>Provided below is an example implementation of the above design. One user with username &ldquo;demo&rdquo; and password “demo” exists. Please do not use this demo in a production environment without https.</p>

<p>To follow along, clone or <a href="https://github.com/imduffy15/aws-credentials-issuer/archive/master.zip">download</a> the code at <a href="https://github.com/imduffy15/aws-credentials-issuer">https://github.com/imduffy15/aws-credentials-issuer</a>.</p>

<h2 id="openid-provider">OpenID Provider</h2>

<p><a href="https://www.keycloak.org/">Keycloak</a> provides an IAM along with OpenID Connect (OIDC) and Security Assertion Markup Language (SAML) interfaces. Additionally, it supports <a href="https://www.keycloak.org/docs/3.0/server_admin/topics/user-federation.html">federation</a> to Active Directory (AD) and Lightweight Directory Access Protocol (LDAP) servers. Keycloak enables organisations to centrally manage employees access to many different services.</p>

<p>The<a href="https://github.com/imduffy15/aws-credentials-issuer"> provided code</a> contains a <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/docker-compose.yaml">docker-compose file</a> that on executing docker-compose up will bring up a keycloak server with its administrative interface accessible at <a href="http://localhost:8080/auth/admin/">http://localhost:8080/auth/admin/</a> using username &ldquo;admin&rdquo; and password “password”.</p>

<p>On the &ldquo;clients&rdquo; screen, a client named “aws-credentials-issuer” is present, users will use this client to generate their JWT tokens. This client is pre-configured to work with the <a href="https://tools.ietf.org/html/rfc6749#section-4.1">Authorization Code Grant</a> for command line interfaces to generate tokens and the <a href="https://tools.ietf.org/html/rfc6749#section-4.2">Implicit Grant</a> for a frontend application.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_3.png" alt="" /></center></p>

<p>Under the &ldquo;aws-credentials-issuer&rdquo; additional roles can be added, these roles must exist on AWS and they must have a trust relationship to the account that will be running the “aws-credentials-issuer”.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_4.png" alt="" /></center></p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_5.png" alt="" /></center></p>

<p>Additionally, these roles must be placed into the users JWT tokens, this is pre-configured under &ldquo;mappers&rdquo;.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_6.png" alt="" /></center></p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_7.png" alt="" /></center></p>

<p>Finally, the role must be assigned to a user. This can be done by navigating to users -&gt; demo -&gt; role mappings and moving the wanted role from &ldquo;available roles&rdquo; to “assigned roles” for the client “aws-credentials-issuer”</p>

<p><img src="/images/managing-access-to-multiple-aws-accounts/image_8.png" alt="" /></p>

<h2 id="aws-credentials-issuer-service">AWS Credentials Issuer Service</h2>

<h3 id="backend">Backend</h3>

<p>The <a href="https://github.com/imduffy15/aws-credentials-issuer">provided code</a> supplies a <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/lambda_handler.py">lambda function</a> which will take care of validating the users JWT token and exchanging it using <code>AssumeRole</code> for AWS Credentials.</p>

<p>This code can be deployed to an AWS account by using the <a href="https://serverless.com/">serverless framework</a> and the <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml">supplied definition</a>. The <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml">definition</a> will create the following:</p>

<ul>
<li><p>An <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L36">IAM role</a> granting AssumeRole rights to the function</p></li>

<li><p>A <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/lambda_handler.py#L67">function</a> for generating AWS API Credentials</p></li>

<li><p>A <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/lambda_handler.py#L36">function</a> for generating <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html">Amazon Federated Sign-On URL</a>s</p></li>
</ul>

<p>With <a href="https://aws.amazon.com/cli/">AWSCLI</a> configured with credentials for the account that the service will run in execute <code>sls deploy</code>, this will deploy the lambda functions and return URLs for executing them.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_9.png" alt="" /></center></p>

<h3 id="frontend">Frontend</h3>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_10.png" alt="" /></center></p>

<p>The <a href="https://github.com/imduffy15/aws-credentials-issuer">provided code</a> supplies a <a href="https://github.com/imduffy15/aws-credentials-issuer/tree/master/ui">frontend</a> which will provide users with a graphical experience for accessing the AWS Web Console or generating AWS API Credentials.</p>

<p>The frontend can be deployed to an S3 bucket using the <a href="https://serverless.com/">serverless framework</a>. Before deploying it some variables must be modified. In the <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L75">serverless definition (</a><a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L75"><code>serverless.yml</code></a><a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/serverless.yml#L75">)</a>, replace &ldquo;ianduffy-aws-credentials-issuer&rdquo; with a desired S3 bucket name and modify <a href="https://github.com/imduffy15/aws-credentials-issuer/blob/master/ui/.env"><code>ui/.env</code></a> to contain your Keycloak and Backend URL as highlighted above. The deployment can be executed with <code>sls client deploy</code>.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_11.png" alt="" /></center></p>

<p>On completion, a URL in the format of <code>http://&lt;bucket-name&gt;.s3-website.&lt;region&gt;.amazonaws.com</code> will be returned. This needs to be supplied to keycloak a redirect URI for the &ldquo;aws-credentials-issuer&rdquo; client.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_12.png" alt="" /></center></p>

<h2 id="usage">Usage</h2>

<h3 id="browser">Browser</h3>

<p>By navigating to the URL of the S3 bucket a user can get access to the AWS Web Console or get API credentials which they can use to manually configure an application.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_13.gif" alt="" /></center></p>

<h3 id="command-line">Command line</h3>

<p>To interact with the &ldquo;aws-credentials-issuer&rdquo; the user must have a valid JWT. This can be done by executing the <a href="https://tools.ietf.org/html/rfc6749#section-4.1">Authorization Code Grant</a> against keycloak.</p>

<p>token-cli can be used to execute the  <a href="https://tools.ietf.org/html/rfc6749#section-4.1">Authorization Code Grant</a> and generate a JWT token, this can be downloaded from the projects <a href="https://github.com/imduffy15/token-cli/releases/tag/v0.0.3">releases page</a>; alternatively, on OSX it can be installed with homebrew brew install imduffy15/tap/token-cli.</p>

<p>Once token-cli is installed it must be configured to run against keycloak, this can be done as follows:</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_14.png" alt="" /></center></p>

<p>Finally, a token can be generated with token-cli token get aws-credentials-issuer -p 9000. On first run the users browser will be opened and they will be required to login, on subsequent runs the token will be cached or refreshed automatically.</p>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_15.gif" alt="" /></center></p>

<p>This token can be used against the &ldquo;aws-credentials-issuer&rdquo; to get AWS API credentials:</p>

<pre><code>curl https://&lt;API-GATEWAY&gt;/dev/api/credentials?role=&lt;ROLE-ARN&gt; \
-H &quot;Authorization: bearer $(token-cli token get aws-credentials-issuer)&quot;
</code></pre>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_16.png" alt="" /></center></p>

<p>Alternatively, a AWS <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html">Amazon Federated Sign-On URL</a> can also be generated:</p>

<pre><code>curl https://&lt;API-GATEWAY&gt;/dev/api/login?role=&lt;ROLE-ARN&gt; \
-H &quot;Authorization: bearer $(token-cli token get aws-credentials-issuer)&quot;
</code></pre>

<p><center><img src="/images/managing-access-to-multiple-aws-accounts/image_17.png" alt="" /></center></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2017/02/26/scala-and-aws-managed-elasticsearch/">Scala and AWS managed ElasticSearch</a>
      </h1>
      <span class="post-date">Feb 26, 2017 &middot; 2 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/programming">programming</a>
      </span>
      
      

<p><a href="https://aws.amazon.com/">AWS</a> offer a <a href="https://aws.amazon.com/elasticsearch-service/">managed ElasticSearch service</a>. It exposes an HTTP endpoint for interacting with ElasticSearch and requires authentication via <a href="https://aws.amazon.com/documentation/iam/">AWS Identity Access Management</a>.</p>

<p><a href="https://github.com/sksamuel/elastic4s">Elastic4s</a> offers a neat DSL and Scala client for ElasticSearch. This post details how to use it with <a href="https://aws.amazon.com/">AWS&rsquo;s</a> <a href="https://aws.amazon.com/elasticsearch-service/">managed ElasticSearch service</a>.</p>

<h2 id="creating-a-request-signer">Creating a request signer</h2>

<p>Using the <a href="https://github.com/inreachventures/aws-signing-request-interceptor">aws-signing-request-interceptor</a> library its easy to create an <a href="https://hc.apache.org/httpcomponents-core-ga/httpcore/apidocs/org/apache/http/HttpRequestInterceptor.html">HttpRequestInterceptor</a> which can be later added to the HttpClient used by <a href="https://github.com/sksamuel/elastic4s">Elastic4s</a> for making the calls to ElasticSearch</p>

<pre><code class="language-scala">private def createAwsSigner(config: Config): AWSSigner = {
  import com.gilt.gfc.guava.GuavaConversions._

  val awsCredentialsProvider = new DefaultAWSCredentialsProviderChain
  val service = config.getString(&quot;service&quot;)
  val region = config.getString(&quot;region&quot;)
  val clock: Supplier[LocalDateTime] = () =&gt; LocalDateTime.now(ZoneId.of(&quot;UTC&quot;))
  new AWSSigner(awsCredentialsProvider, region, service, clock)
}
</code></pre>

<h2 id="creating-an-http-client-and-intercepting-the-requests">Creating an HTTP Client and intercepting the requests</h2>

<p>The ElasticSearch <a href="https://github.com/elastic/elasticsearch/blob/master/client/rest/src/main/java/org/elasticsearch/client/RestClientBuilder.java#L230">RestClientBuilder</a> allows for registering a callback to modify the customise the <a href="http://hc.apache.org/httpcomponents-asyncclient-dev/httpasyncclient/apidocs/org/apache/http/impl/nio/client/HttpAsyncClientBuilder.html#addInterceptorLast(org.apache.http.HttpResponseInterceptor)">HttpAsyncClientBuilder</a> enabling registering the interceptor to sign the requests.</p>

<p>The callback can be created by implementing the HttpClientConfigCallback interface as follows:</p>

<pre><code class="language-scala">private val esConfig = config.getConfig(&quot;elasticsearch&quot;)

private class AWSSignerInteceptor extends HttpClientConfigCallback {
  override def customizeHttpClient(httpClientBuilder: HttpAsyncClientBuilder): HttpAsyncClientBuilder = {
    httpClientBuilder.addInterceptorLast(new AWSSigningRequestInterceptor(createAwsSigner(esConfig)))
  }
}
</code></pre>

<p>Finally, an <a href="https://github.com/sksamuel/elastic4s">Elastic4s</a> client can be created with the interceptor registered:</p>

<pre><code class="language-scala">private def createEsHttpClient(config: Config): HttpClient = {
  val hosts = ElasticsearchClientUri(config.getString(&quot;uri&quot;)).hosts.map {
    case (host, port) =&gt;
      new HttpHost(host, port, &quot;http&quot;)
  }

  log.info(s&quot;Creating HTTP client on ${hosts.mkString(&quot;,&quot;)}&quot;)

  val client = RestClient.builder(hosts: _*)
    .setHttpClientConfigCallback(new AWSSignerInteceptor)
    .build()
  HttpClient.fromRestClient(client)
}
</code></pre>

<p>Full Example on <a href="https://github.com/imduffy15/scala-aws-hosted-es/blob/master/src/main/scala/Run.scala">GitHub</a></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2016/11/27/azure-bug-bounty-root-to-storage-account-administrator/">Azure bug bounty Root to storage account administrator</a>
      </h1>
      <span class="post-date">Nov 27, 2016 &middot; 2 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/cloud">Cloud</a><a class="label" href="http://ianduffy.ie/categories/security">Security</a><a class="label" href="http://ianduffy.ie/categories/azure">Azure</a>
      </span>
      
      <p>In my previous blog post <a href="http://ianduffy.ie/blog/2016/11/26/azure-bug-bounty-pwning-red-hat-enterprise-linux/">Azure bug bounty Pwning Red Hat Enterprise Linux</a> I detailed how it was possible to get administrative access to the Red Hat Update Infrastructure consumed by <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> virtual machines booted from the <a href="https://azure.microsoft.com">Microsoft Azure</a> Marketplace image. In theory, if exploited one could have gained root access to all virtual machines consuming the repositories by releasing an updated version of a common package and waiting for virtual machines to execute <code>yum update</code>.</p>

<p>As an attacker, this would have granted access to every piece of data on the compromised virtual machines. Sadly, the attack vector is actually much more widespread than this. Given some poor implementation within the mandatory <a href="https://azure.microsoft.com">Microsoft Azure</a> Linux Agent (WaLinuxAgent) one is able to obtain the administrator API keys to the storage account used by the virtual machine for debug log shipping purposes, at the time of research this storage account defaulted to one shared by multiple virtual machines.</p>

<p>At the time of research, the <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> image available on the <a href="https://azure.microsoft.com">Microsoft Azure</a> Marketplace came with WaLinuxAgent 2.0.16. When a virtual machine was created with the &ldquo;Linux diagnostic extension&rdquo; enabled the API key for access to the specified storage account was written to <code>/var/lib/waagent/Microsoft.OSTCExtensions.LinuxDiagnostic-2.3.9007/xmlCfg.xml</code>.</p>

<p>Once acquired one can simply use the <a href="https://github.com/Azure/azure-xplat-cli">Azure Xplat-CLI</a> to interact the storage account:</p>

<pre><code>export AZURE_STORAGE_ACCOUNT=&quot;storage_account_name_as_per_xmlcfg&quot;
export AZURE_STORAGE_ACCESS_KEY=&quot;storage_account_access_key_as_per_xmlcfg&quot;
azure storage container list # acquire some container name
azure storage blob list # provide the container name
# Copy, download, upload, delete any blobs available across any containers you can access.
</code></pre>

<p>If the storage account was used by multiple virtual machines there is potential to download their virtual hard disks.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2016/11/26/azure-bug-bounty-pwning-red-hat-enterprise-linux/">Azure bug bounty Pwning Red Hat Enterprise Linux</a>
      </h1>
      <span class="post-date">Nov 26, 2016 &middot; 5 minute read
      
      <br/>
      <a class="label" href="http://ianduffy.ie/categories/cloud">Cloud</a><a class="label" href="http://ianduffy.ie/categories/security">Security</a><a class="label" href="http://ianduffy.ie/categories/azure">Azure</a>
      </span>
      
      

<p><em>TL;DR Acquired administrator level access to all of the <a href="https://azure.microsoft.com">Microsoft Azure</a> managed <a href="https://access.redhat.com/documentation/en/red-hat-update-infrastructure/3.0.beta.1/paged/system-administrator-guide/chapter-1-about-red-hat-update-infrastructure">Red Hat Update Infrastructure</a> that supplies all the packages for all <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> instances booted from the Azure marketplace.</em></p>

<p>I was tasked with creating a machine image of <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> that was compliant to the <a href="https://www.stigviewer.com/stig/red_hat_enterprise_linux_6/">Security Technical Implementation guide defined by the Department of Defense</a>.</p>

<p>This machine image was to be used for both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a>. Both of which offer marketplace images which had a metered billing pricing model[1][2]. Ideally, I wanted my custom image to be billed under the same mechanism, as such the virtual machines would be able to consume software updates from a local <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> repository owned and managed by the cloud provider.</p>

<p>Both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a> utilise a deployment of <a href="https://access.redhat.com/documentation/en/red-hat-update-infrastructure/3.0.beta.1/paged/system-administrator-guide/chapter-1-about-red-hat-update-infrastructure">Red Hat Update Infrastructure</a> for supplying this functionality.</p>

<p>This setup requires two main parts:</p>

<h3 id="red-hat-update-appliance">Red Hat Update Appliance</h3>

<p>There is only one Red Hat Update Appliance per <a href="https://access.redhat.com/documentation/en/red-hat-update-infrastructure/3.0.beta.1/paged/system-administrator-guide/chapter-1-about-red-hat-update-infrastructure">Red Hat Update Infrastructure</a> installation, however, both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a> create one per region.</p>

<p>The Red Hat Update Appliance is responsible for:</p>

<ul>
<li>Downloading new packages from the Red Hat CDN. It is the only component that requires a connection back to Red Hat.</li>
<li>Copying new packages to each content delivery server</li>
<li>Supplying a <a href="http://pulpproject.org/">REST API and command line interface for management of software repositories</a></li>
</ul>

<p><strong>The Red Hat Update Appliance does not need to be exposed to the repository clients.</strong></p>

<h3 id="content-delivery-server">Content Delivery server</h3>

<p>The content delivery server(s) provide the yum repositories that clients connect to for updated packages.</p>

<h2 id="achieving-metered-billing">Achieving metered billing</h2>

<p>Both <a href="https://aws.amazon.com/">Amazon Web Services</a> and <a href="https://azure.microsoft.com">Microsoft Azure</a> use SSL certifications for authentication against the repositories.</p>

<p>However, these are the same SSL certificates for every instance.</p>

<p>On <a href="https://aws.amazon.com/">Amazon Web Services</a> having the SSL certificates is not enough, you must have booted your instance from an AMI that had an associated billing code. It is this billing code that ensures you pay the extra premium for running <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a>.</p>

<p>On Azure it remains undefined how they manage to track billing. At the time of research, it was possible to copy the SSL certificates from one instance to another and successfully authenticate. Additionally, if you duplicated a <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> virtual hard disk and created a new instance from it all billing association seemed to be lost but repository access was still available.</p>

<h2 id="where-azure-failed">Where Azure Failed</h2>

<p>On Azure to setup repository connectivity, they provide an RPM with the necessary configuration. In the older version of their agent, it is responsible for this task [3].  The installation script it references comes from the following <a href="http://rhuirpm.blob.core.windows.net/script/rhui.tar.gz">archive</a>. If you expand this archive you will find the client configuration for each region.</p>

<p>By browsing the metadata of the RPMs we can discover some interesting information:</p>

<pre><code class="language-bash">$ rpm -qip RHEL6-2.0-1.noarch.rpm
Name        : RHEL6                        Relocations: (not relocatable)
Version     : 2.0                               Vendor: (none)
Release     : 1                             Build Date: Sun 14 Feb 2016 06:40:54 AM UTC
Install Date: (not installed)               Build Host: westeurope-rhua.cloudapp.net
Group       : Applications/Internet         Source RPM: RHEL6-2.0-1.src.rpm
Size        : 20833                            License: GPLv2
Signature   : (none)
URL         : http://redhat.com
Summary     : Custom configuration for a cloud client instance
Description :
Configurations for a client to connect to the RHUI infrastructure
</code></pre>

<p>As you can see, the build host enables us to discover all of the Red Hat Update Appliances:</p>

<pre><code class="language-bash">$ host westeurope-rhua.cloudapp.net
westeurope-rhua.cloudapp.net has address 104.40.209.83

$ host eastus2-rhua.cloudapp.net
eastus2-rhua.cloudapp.net has address 13.68.20.161

$ host southcentralus-rhua.cloudapp.net
southcentralus-rhua.cloudapp.net has address 23.101.178.51

$ host southeastasia-rhua.cloudapp.net
southeastasia-rhua.cloudapp.net has address 137.116.129.134
</code></pre>

<p>At the time of research, all of servers were exposing their REST APIs over HTTPs.</p>

<p>The URL to the archive containing these RPMs was discovered a package labeled PrepareRHUI on available on any <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> Box running on <a href="https://azure.microsoft.com">Microsoft Azure</a>.</p>

<pre><code class="language-bash">$ yumdownloader PrepareRHUI
$ rpm -qip PrepareRHUI-1.0.0-1.noarch.rpm
Name        : PrepareRHUI                  Relocations: (not relocatable)
Version     : 1.0.0                             Vendor: Microsoft Corporation
Release     : 1                             Build Date: Mon 16 Nov 2015 06:13:21 AM UTC
Install Date: (not installed)               Build Host: rhui-monitor.cloudapp.net
Group       : Unspecified                   Source RPM: PrepareRHUI-1.0.0-1.src.rpm
Size        : 770                              License: GPL
Signature   : (none)
Packager    : Microsoft Corporation &lt;xiazhang@microsoft.com&gt;
Summary     : Prepare RHUI installation for Redhat client
Description :
PrepareRHUI is used to prepare RHUI installation for before making a Redhat image.
</code></pre>

<p>The build host is interesting <code>rhui-monitor.cloudapp.net</code>, at the time of research running a port scan revealed an application running on port 8080.</p>

<p><img src="/monitor.png" alt="Microsoft Azure RHUI Monitoring tool" /></p>

<p>Despite the application requiring username and password based authentication, It was possible to execute a run of their &ldquo;backend log collector&rdquo; on a specified content delivery server. When the collector service completed the application supplied URLs to archives which contain multiple logs and configuration files from the servers.</p>

<p>Included within these archives was an SSL certificate that would grant full administrative access to the Red Hat Update Appliances [4].</p>

<p><img src="/directory.png" alt="Pulp admin keys for Microsoft Azure's RHUI" /></p>

<p>At the time of research all <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> virtual machines booted from the Azure Marketplace image had the following additional repository configured:</p>

<pre><code>[rhui-PA]
name=Packages for Azure
mirrorlist=https://eastus2-cds1.cloudapp.net/pulp/mirror/PA
enabled=1
gpgcheck=0
sslverify=1
sslcacert=/etc/pki/rhui/ca.crt
</code></pre>

<p>Given no gpgcheck is enabled, with full administrative access to the <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red Hat Enterprise Linux</a> Appliance REST API one could have uploaded packages that would be acquired by client virtual machines on their next yum update.</p>

<p>The issue was reported in accordance to the <a href="https://technet.microsoft.com/en-us/library/dn800983.aspx">Microsoft Online Services Bug Bounty terms</a>. Microsoft agreed it was a vulnerability in their systems. Immediate action was taken to prevent public access to <code>rhui-monitor.cloudapp.net</code>. Additionally, they eventually prevented public access to the Red Hat Update Appliances and they claim to have rotated all secrets.</p>

<p>[1] <a href="https://azure.microsoft.com/en-in/pricing/details/virtual-machines/red-hat/">https://azure.microsoft.com/en-in/pricing/details/virtual-machines/red-hat/</a></p>

<p>[2] <a href="https://aws.amazon.com/partners/redhat/">https://aws.amazon.com/partners/redhat/</a></p>

<p>[3] <a href="https://github.com/Azure/azure-linux-extensions/blob/master/Common/WALinuxAgent-2.0.16/waagent#L2891">https://github.com/Azure/azure-linux-extensions/blob/master/Common/WALinuxAgent-2.0.16/waagent#L2891</a></p>

<p>[4] <a href="https://fedorahosted.org/pulp/wiki/Certificates">https://fedorahosted.org/pulp/wiki/Certificates</a></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://ianduffy.ie/blog/2016/06/28/hello-world/">Hello World</a>
      </h1>
      <span class="post-date">Jun 28, 2016 &middot; 1 minute read
      
      <br/>
      
      </span>
      
      <pre><code class="language-ruby">resource &quot;null_resource&quot; &quot;hello_world&quot; {
    provisioner &quot;local-exec&quot; {
        command = &quot;echo 'Hello World'&quot;
    }
}
</code></pre>

<p>Interested in automation and the HashiCorp suite of tools? If so you&rsquo;ll love this blog.
Through different posts we will explore lots different automation tasks utilising both
public and private cloud with the HashiCorp toolset.</p>

<p>Thanks,
Ian.</p>

      
    </div>
    
    

  </div>
</div>


<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>

